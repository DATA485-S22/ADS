---
title: "Tree-based models"
output:   
  html_document: 
    highlight: tango
    theme: yeti
    toc: yes
    toc_float: true
---


# Goals

* Learn what a decision tree is, how it's constructed and how to read one
* Understand model enhancements like bagging and boosting
* Fit a regression and classification tree on data using `caret`. 
* Describe the benefits and drawbacks for using a Random Forest instead of a single Decision Tree

# Assignments

* Reading: ISLR Ch 8
* Homework: [[HTML]](hw09-CART-assignment.html) [[RMD]](hw09-CART-assignment.Rmd)


----

# Learning - Theory

Answer the following questions. The questions are split into groups following the videos, which does not always align with numbered sections in the text. 

## The Basics of Decision Trees (ISLR 8.1) 

### Building Regression Trees 

[[Video 1 14:37]](https://www.youtube.com/watch?v=6ENTbK3yQUQ&list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh) 

1. What does it mean to _segment_ the data? What do we call this in MATH 456? 
2. Are tree based methods supervised or unsupervised methods? 
3. What does CART stand for? 
4. What determines if a record goes to the left or right of a split? (i.e. what records go left, what go right?) 
5. What do the values in the _internal_ and _terminal_ nodes represent? 
6. How can you tell which variables are most 'important' in a tree model? 
7. What is the _feature space_? Compare this to the term _predictor space_. Do this in your own words. You may want to look into other sources of information (outside the book) for something that fits into your mental model. 
8. Explain how _recursive binary splitting_ works without using equations. 
9. Explain why this process has this name. That is, what about this process is _recursive_? (don't use the word in the definition either), why _binary_?
10. Why its considered _top down_ and _greedy_. 
11. What is a _stopping criterion_? Give an example of one. 
12. Why are decision trees considered a statistical _learning_ model? Where does the algorithmic learning occur? 


### Pruning Trees 

[[Video 2 11:45]](https://www.youtube.com/watch?v=GfPR7Xhdokc&list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh&index=2)

1. How are predictions of new values made? 
2. Why is pruning necessary? Why can't we just build a short tree by stopping early?
3. How is _Cost complexity pruning_ similar to LASSO? 
4. What is the penalty function? Report this in symbols, not words, but define each symbol in words.
5. How is $\alpha$ chosen? 
6. What information do you get out of Figure 8.5? If you were to build this plot as part of your analysis (just the black and green lines), what information can you draw from it? 


## Classification Trees (ISLR 8.1.2) 

[[Video 3 11:00]](https://www.youtube.com/watch?v=hPEJoITBbQ4&list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh&index=3)

1. How do you make predictions from a classification tree? 
2. What is the _classification error rate_? 
3. What does _node purity_ mean? Why is it important (it may be helpful to read through the example before answering this question).
4. Is a large or small _Gini index_ preferred? 
5. If prediction accuracy is the goal of tree building, what measure should be used to prune the tree? 

## Advantages and Disadvantages of Trees (ISLR 8.1.4)

1. Name one advantage and disadvantage of trees when compared to classical methods. 



## Bagging, Random Forests, Boosting (ISLR 8.2)

### Bagging (ISLR 8.2.1) 

[[Video 13:45]](https://www.youtube.com/watch?v=lq_xzBRIWm4&list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh&index=4)

1. Bagging is a general-purpose procedure for reducing (bias or variance?) of a statistical learning method
2. Explain how bagging works.  
3. What is a _majority vote_ in the context of classification? 
4. What is an out of bag error estimate? How does it compare to k-fold CV/LOOCV?


### Random Forests
1. What's the main difference between building a Random Forest vs simply bagged trees? 
2. How is the Gini index used to generate a summary measure of importance for each predictor in a Random Forest? 
3. In the gene expression example (video), what critiera do they use to pre-screen variables to go into the model, and why? 
4. Why can we say that Random Forests is equivlenant to bagging trees when $m=p$?
5. Can you overfit a Random Forest model by constructing too many trees? 


### Boosting (ISLR 8.2.3)

[[Video 12:02]](https://www.youtube.com/watch?v=U3MdBNysk9w&list=PL5-da3qGB5IB23TLuA8ZgVGC8hV8ZAdGh&index=5)

1. How is boosting a sequential method? 
2. In Boosting, why are the trees are grown on the residuals? 
3. What are the three tuning parameters for boosting? 

----

# Learning - Application

Using your UCI data from the last homework, and using the `caret` package, 

```{r}
# annotated package loading from: https://bradleyboehmke.github.io/HOML/DT.html 
# Helper packages
library(dplyr)       # for data wrangling
library(ggplot2)     # for awesome plotting

# Modeling packages
library(rpart)       # direct engine for decision tree application
library(caret)       # meta engine for decision tree application

# Model interpretability packages
library(rpart.plot)  # for plotting decision trees
library(vip)         # for feature importance



load("breast_cancer_UCI.Rdata")
```

Recall this is a dataset with continuous measurements on breast tumors, with a categorical response variable of either `B`enign or `M`alignant. 

## Preprocess

Split into testing/training. Set train control parameters (5 fold CV), and preprocessing steps (normalizing the data). 

```{r}
train.idx <- createDataPartition(bc$diagnosis, p = .6, list = FALSE)

train.dta <- bc[train.idx,]
test.dta  <- bc[-train.idx,]

fitControl <- trainControl(method="cv", number=5)
PreProc    <- c("center", "scale")
```

## 1. Build a decision tree using the `rpart` method (recursive partioning). Requires the `rpart` package. 

```{r}
bc.tree <- train(diagnosis ~ ., 
                  data=train.dta, 
                  method="rpart", 
                  trControl = fitControl, 
                  preProcess = PreProc)
```

The basic output shows a recap of what the data looks like (n, p, classes), the preprocessing and resampling processes, as well as the tuning parameters and which were chosen to use in the final model. 
```{r}
bc.tree
```

Calling a `summary` on the tree object gives us more information. 

```{r}
summary(bc.tree)
```

A *lot* more. Specifically it is showing [1] the data set (not ideal with large datasets), [2] the variable importance measures, and [3] information on each node such as what the splits were, how many observations are to either side of the split, and the predicted probability of the outcome for that node. 

Let's see how to get this information out in a more concise manner. Like plotting the final tree!

```{r}
plot(bc.tree$finalModel, uniform=TRUE)
text(bc.tree$finalModel, use.n = TRUE, all=TRUE, cex=.8)
```

How about a fancier version? Perhaps more readable? 

```{r}
library(rattle)
fancyRpartPlot(bc.tree$finalModel)

rpart.plot(bc.tree)
```

Okay prettier, but what do the numbers mean?










Prune it. Make a plot. Interpret the results. Calculate test sample accuracy. 


2. Grow a forest. Plot the variable importance. Interpret the results. Calculate test sample accuracy. How does it compare to your models from last homework? 
3. Compare RF to GBM

    
    
    
    
    
    
    
    
    
