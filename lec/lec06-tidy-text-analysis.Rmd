---
title: "Tidy Text Analysis"
output:   
  html_document: 
    highlight: tango
    theme: yeti
    toc: yes
    toc_float: true
---


# Overview

* Operationalize project research questions into data tasks
* Text processing and Sentiment analysis

# Goals

* Operationalize what data/format each group needs for their project question
* Understand the minimum necessary information about text processing and sentiment analysis. 
* Identify what parts of text analysis will be needed for your groups project
* Replicate the steps needed for text processing and sentiment scoring to your twitter project data (even if it's not part of your research question).


# Learning resources

This work follows from the [Text mining with R](https://www.tidytextmining.com/) book. These notes serve as a demonstration of how these topics apply to our twitter data. 

* Tidytext book and vignette (recommended)
    - [Vignette](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html)
    - [Text mining with R book](https://www.tidytextmining.com/)
* [Sentiment Analysis](https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html) package vignette. 



```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
```

## Data Cleaning
Read in data. Extract tweet text into a new tibble.
```{r}
cf <- readRDS("C:/Users/Robin/Google Drive/Teaching/POLS 624 S20/Twitter project docs/data/campfire-tweets-2020-04-27.Rds")
tweet <- tibble(status_id=cf$status_id, text=cf$text)
tweet$text[1]
```

Tokenization using the `unnest_tokens()` function. 
```{r}
tweet %>% unnest_tokens(word, text) %>% print(n=15)
```

Notice when we unnested the tokens, it removed the `#` and `@` symbols. We can use a custom token function for tweets. 
We should also remove symbols such as `&`, `>` and `<`. 

```{r}
remove_reg <- "&amp;|&lt;|&gt;"
tweet_words <- tweet %>% 
                  mutate(text = str_remove_all(text, remove_reg)) %>%
                  unnest_tokens(word, text, token="tweets")
tweet_words %>% print(n=15)
```

Remove stop words - what's in the stop words lexicon anyhow? 
```{r}
data("stop_words")
head(stop_words)
table(stop_words$lexicon)
```

```{r}
tweet_words_nostop <- tweet_words %>% anti_join(stop_words)
head(tweet_words_nostop)
```

Most frequent words? 
```{r}
tweet_words_nostop %>% count(word, sort = TRUE)
```

## Exploration
Top tags
```{r}
tweet_words_nostop %>% filter(str_detect(word, "^#")) %>% count(word, sort=TRUE)
```

Top mentions
```{r}
tweet_words_nostop %>% filter(str_detect(word, "^@")) %>% count(word, sort=TRUE)
```

@doggietatoo???? Need to look into some of these users/mentions. 
Or better yet, see if we can find a way to identify using a model which tweets are actually about the disaster. 

---

## Sentiments

Read the notice about [proper licensing of lexicons](https://www.tidytextmining.com/sentiment.html). We will have to cite this in any work that uses this lexicon. I am choosing AFINN because it creates a score from -5 to 5 (part of the `textdata` package). Each lexicon mentioned in this text has it's advantages. You will have to choose with your group which one you will use. The Text mining with R [Chapter 2.3](https://www.tidytextmining.com/sentiment.html#comparing-the-three-sentiment-dictionaries) shows how you can use all three dictionaries to compare sentiment application methods to the same tweet. 

```{r}
library(textdata)
```

When learning a new thing, i always do it in stages so I can see exactly how the data is being modified.
```{r}
ts1 <- tweet_words_nostop %>% 
          inner_join(get_sentiments("afinn"))
head(ts1)
```

We are going to want the overall sentiment per tweet as well, so we employ group_by and summarize tactic.  
```{r}
ts2 <- ts1 %>% group_by(status_id) %>% summarize(sentiment=sum(value))
head(ts2)
```

Since these still have the ID's attached (on purpose), we can merge this score back on to the original full tweet data. 

```{r}
cf2 <- cf %>% left_join(ts2, by='status_id')
names(cf2)
```

Simple comparison of sentiment by verified vs not (may not be of interest, but it was an binary variable that I had easy access to).
```{r}
ggplot(cf2, aes(x=sentiment, col=verified)) + geom_density(lwd=2) + theme_minimal()
```

See the rest of Chapter 2 in the Text mining with R book for other examples of what you can do with words and sentiments. 


## Considerations

* Not all tweets (especially for COVID-19) are in English
* Records contain original, and re-tweeted content. The frequency of a word said only once, but retweeted thousands of times, may appear to be more frequent than it really is. Decide what type of tweets your group wants to analyze and apply that filter right away. 
* You are likely going to be applying sentiments to the tweets at the project level, not beforehand. 
    - We could, but with the variety of lexicons, and what can be done with the word level datasets, that some of this will happen at the project level anyhow. 
* Don't forget to check for abnormalities in the tweets, especially for the campfire. We can't guarantee all tweets are about this one disaster. 
* We could get into the weeds here. This is a topic of an [entire dissertation topic](https://scholarcommons.usf.edu/cgi/viewcontent.cgi?article=8925&context=etd). So don't set your sights too high for this class project. 

